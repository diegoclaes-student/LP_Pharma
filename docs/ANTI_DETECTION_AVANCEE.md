# üõ°Ô∏è Guide Complet Anti-D√©tection et Evasion de Blocage

## Table des mati√®res
1. [Diagnostique des blocages](#diagnostique)
2. [Solutions par niveau de risque](#solutions)
3. [Strat√©gies compl√®tes](#strategies)
4. [Impl√©mentation recommand√©e](#implementation)
5. [Monitoring et alertes](#monitoring)

---

## üîç Diagnostique des Blocages {#diagnostique}

### Pourquoi Farmaline et NewPharma bloquent ?

| Site | Type de blocage | D√©tection | Difficult√© |
|------|-----------------|-----------|-----------|
| **NewPharma** | 403 Forbidden | IP + Pattern | Tr√®s √©lev√©e |
| **Farmaline** | 403 Forbidden | User-Agent + Pattern | √âlev√©e |
| **Medi-Market** | ‚úÖ Permissif | Minimal (d√©lais respect√©s) | Faible |
| **Multipharma** | ‚úÖ Permissif | Minimal (async ok) | Faible |

### Signaux de d√©tection

```
‚ùå D√âTECT√â = Erreur 403/429 r√©guli√®rement
- Trop de requ√™tes par seconde
- User-Agent anormal
- Pattern de requ√™tes trop r√©gulier
- Pas de pauses entre sessions
- IP uniquement (datacenter)
- Headers vides/incomplets
```

---

## üéØ Solutions par Niveau de Risque {#solutions}

### Niveau 1: Sans Risque (D√©j√† impl√©ment√© ‚úÖ)
**Co√ªt**: Gratuit | **Effort**: Faible | **Efficacit√©**: 80%

```
‚úÖ Connection pooling (HTTPAdapter)
‚úÖ Headers r√©alistes (User-Agent, Accept-Language)
‚úÖ D√©lais al√©atoires (0.1-0.8s)
‚úÖ Pauses p√©riodiques (tous les 50 produits)
‚úÖ Timeout court (fail-fast)
‚úÖ Async pour concurrence efficace
```

**Quand l'utiliser**: ‚úÖ Medi-Market, Multipharma
**Limitations**: ‚ùå Inefficace sur Farmaline/NewPharma

---

### Niveau 2: Mod√©r√© (Juste au-dessus du safe)
**Co√ªt**: Gratuit | **Effort**: Moyen | **Efficacit√©**: 85%

#### 2.1 Rotation de User-Agents avanc√©e
```python
# Au lieu d'un seul User-Agent fixe
USER_AGENTS = [
    # Chrome versions r√©centes
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    
    # Firefox versions r√©centes
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7; rv:121.0) Gecko/20100101 Firefox/121.0",
    
    # Safari
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    
    # Edge
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    
    # Mobile (parfois moins bloqu√©)
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Linux; Android 14) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
]

# Inclure aussi les Accept-Language vari√©s
ACCEPT_LANGUAGES = [
    "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
    "fr;q=0.9,en-US;q=0.8,en;q=0.7",
    "en-US,en;q=0.9,fr;q=0.8",
    "nl-BE,nl;q=0.9,fr;q=0.8",  # N√©erlandais (Belgique)
]
```

#### 2.2 D√©lais intelligents
```python
# Au lieu de d√©lais fixes
import time
import random

# D√©lais variables selon contexte
DELAY_SHORT = random.uniform(0.1, 0.3)   # Entre deux recherches simples
DELAY_MEDIUM = random.uniform(0.5, 1.2)  # Entre deux produits
DELAY_LONG = random.uniform(2, 5)        # Pause de s√©curit√©

# Ajouter de l'al√©a r√©aliste (comportement humain)
def human_like_delay():
    """Simule un comportement humain avec pauses irr√©guli√®res"""
    choice = random.random()
    if choice < 0.1:  # 10% de chance: pause longue (distraction)
        return random.uniform(5, 15)
    elif choice < 0.3:  # 20% de chance: pause moyenne
        return random.uniform(2, 5)
    else:  # 70% de chance: pause courte
        return random.uniform(0.5, 2)

time.sleep(human_like_delay())
```

#### 2.3 Rotation d'adresses IP locales
```python
# Si vous avez plusieurs connexions locales (VPN, proxy local)
PROXIES = [
    None,  # Pas de proxy (votre IP)
    "socks5://127.0.0.1:9050",  # Tor local (lent mais tr√®s anonyme)
    "http://proxy1.local:8080",
    "http://proxy2.local:8080",
]

# Rotation al√©atoire
import random
proxy = random.choice(PROXIES)
response = session.get(url, proxies={"https": proxy} if proxy else {})
```

**Quand l'utiliser**: Pour tenter Farmaline une 2e fois
**Limitations**: Besoin de VPN/proxy local, plus lent

---

### Niveau 3: √âlev√© (Solutions sophistiqu√©es)
**Co√ªt**: 10-100‚Ç¨/mois | **Effort**: √âlev√© | **Efficacit√©**: 90-95%

#### 3.1 Service de Proxy R√©sidentiels
```python
# Utiliser un service comme Bright Data, ScraperAPI, Oxylabs
# Chaque requ√™te sort d'une IP diff√©rente (r√©sidentielle = r√©elle personne)

import requests

# Option A: ScraperAPI (simple)
SCRAPER_API_KEY = "votre_cl√©"

def scrape_with_scraper_api(url):
    payload = {
        'api_key': SCRAPER_API_KEY,
        'url': url,
        'render': 'false',
        'country_code': 'be',  # Proxy Belgique
    }
    return requests.get('http://api.scraperapi.com', params=payload)

# Option B: Bright Data (contr√¥le fin)
proxy_url = "http://user:password@proxy.provider:port"
response = session.get(url, proxies={"https": proxy_url})

# Prix ScraperAPI: ~0.5-1‚Ç¨ par 1000 requ√™tes
# Prix Bright Data: ~10-50‚Ç¨/mois selon volume
```

**Avantages**:
- ‚úÖ Une IP diff√©rente √† chaque requ√™te
- ‚úÖ IPs r√©sidentielles (moins suspectes)
- ‚úÖ G√©olocalisation (peut choisir pays)
- ‚úÖ Gestion automatique des rotations

**D√©savantages**:
- ‚ùå Co√ªteux
- ‚ùå Plus lent (requ√™te passe par proxy)
- ‚ùå Limite de volume par mois

**Fournisseurs recommand√©s**:
1. **ScraperAPI** - Simple, pas de config proxy
2. **Bright Data** - Plus de contr√¥le, meilleur support
3. **Oxylabs** - Tr√®s fiable pour sites complexes
4. **Luminati** - De Bright Data, m√™me tech

---

#### 3.2 Browser Automation avec Playwright
```python
# Les sites d√©tectent les bots en testant JavaScript
# Solution: utiliser un navigateur r√©el

from playwright.async_api import async_playwright
import asyncio

async def scrape_with_playwright(url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            # D√©sactiver d√©tection headless
            args=['--disable-blink-features=AutomationControlled']
        )
        
        # Context avec options r√©alistes
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            locale="fr-BE",
            timezone_id="Europe/Brussels",
            # Simuler g√©olocalisation
            geolocation={"latitude": 50.8503, "longitude": 4.3517},
            permissions=["geolocation"],
        )
        
        page = await context.new_page()
        
        # Attendre que la page se charge compl√®tement
        await page.goto(url, wait_until="networkidle")
        
        # R√©cup√©rer le HTML rendu (apr√®s JavaScript)
        content = await page.content()
        
        await browser.close()
        return content

# Utiliser dans le scraper
# html = await scrape_with_playwright(url)
```

**Avantages**:
- ‚úÖ Ex√©cute JavaScript
- ‚úÖ Bloque les d√©tecteurs de bots
- ‚úÖ Contr√¥le fin du comportement
- ‚úÖ Peut cliquer, remplir formulaires

**D√©savantages**:
- ‚ùå Tr√®s lent (5-10x plus)
- ‚ùå Consomme beaucoup RAM
- ‚ùå Complexe √† debugger

**Quand l'utiliser**: Seulement si Farmaline/NewPharma exigent JavaScript

---

#### 3.3 Cloudflare Bypass
Farmaline/NewPharma utilisent probablement Cloudflare pour WAF.

```python
# Option 1: cloudflare-scrape (mais obsol√®te)
# Option 2: cloudscraper
import cloudscraper

scraper = cloudscraper.create_scraper()
response = scraper.get(url)  # Contourne automatiquement Cloudflare

# Option 3: Playwright + Cloudflare bypass
# (Playwright passe souvent naturellement)
```

**Installation**:
```bash
pip install cloudscraper
# ou
pip install cf-clearance
```

---

### Niveau 4: Extr√™me (Solutions l√©gales mais sophistiqu√©es)
**Co√ªt**: 100‚Ç¨+ | **Effort**: Tr√®s √©lev√© | **Efficacit√©**: 98%+

#### 4.1 API officielle
```python
# Contacter le site pour une API officielle
# Farmaline/NewPharma pourraient avoir une API B2B

# Avantages:
# ‚úÖ L√©gal et garanti
# ‚úÖ Plus rapide et fiable
# ‚úÖ Support officiel
# ‚ùå Acc√®s limit√© ou payant
```

#### 4.2 H√©bergement en datacenter belgique
```python
# Louer un VPS en Belgique
# Les sites bloquent souvent les datacenters non-belgiques
# VPS √† partir de 5-10‚Ç¨/mois chez Linode, DigitalOcean, OVH
```

#### 4.3 Selenium + UndetectedChromeDriver
```python
from undetected_chromedriver import Chrome
import time

driver = Chrome()
driver.get(url)
time.sleep(2)  # Laisser charger

# C'est un navigateur normal, tr√®s difficile √† d√©tecter
html = driver.page_source
driver.quit()
```

---

## üìã Strat√©gies Compl√®tes {#strategies}

### Strat√©gie A: "Safe & Slow" (Recommand√© initialement)
**Cible**: Medi-Market ‚úÖ + Multipharma ‚úÖ
**Temps**: 1 min | **Co√ªt**: 0‚Ç¨ | **Risque**: 0%

```
1. Garder la config actuelle (async + adaptive delays)
2. Augmenter d√©lais de s√©curit√© (2s entre produits)
3. Ajouter variation User-Agents
4. Arr√™ter si erreur 429 apr√®s 2 tentatives
```

---

### Strat√©gie B: "Fast & Agile" (Revenir aux 2 sites)
**Cible**: Farmaline ‚ö†Ô∏è + NewPharma ‚ö†Ô∏è
**Temps**: 2 min | **Co√ªt**: 5-10‚Ç¨ | **Risque**: Moyen

```
Niveau 2 + Niveau 3.1:
1. Rotation User-Agents compl√®te (8 agents)
2. Human-like delays avec pauses irr√©guli√®res
3. Ajouter Accept-Language variables
4. ScraperAPI pour 20% des requ√™tes Farmaline
5. Monitoring des erreurs 403
6. Backoff exponentiel: 1s ‚Üí 5s ‚Üí 30s
```

**Impl√©mentation**:
```python
# Pseudocode
def scrape_farmaline_safe(cnk_list):
    attempt = 0
    max_attempts = 3
    
    while attempt < max_attempts:
        try:
            # 80% requ√™tes directes, 20% via ScraperAPI
            if random.random() < 0.8:
                response = session.get(url, headers=rotate_headers())
            else:
                response = scrape_with_scraper_api(url)
            
            if response.status_code == 200:
                return parse(response)
            elif response.status_code == 403:
                raise BlockedException()
                
        except BlockedException:
            attempt += 1
            wait = 2 ** attempt  # 2s, 4s, 8s...
            print(f"‚è≥ Blocked, wait {wait}s")
            time.sleep(wait)
    
    return None
```

---

### Strat√©gie C: "Maximum Coverage" (Tous les sites)
**Cible**: Medi-Market ‚úÖ + Multipharma ‚úÖ + Farmaline ‚úÖ + NewPharma ‚úÖ
**Temps**: 3-5 min | **Co√ªt**: 20-50‚Ç¨/mois | **Risque**: Faible si bien configur√©

```
Niveau 3 complet:
1. ScraperAPI pour Farmaline/NewPharma
2. Playwright fallback pour pages complexes
3. Cloudscraper pour Cloudflare
4. G√©olocalisation Belgique
5. D√©lais adaptatifs
6. Monitoring complet
```

**Co√ªts mensuels**:
- ScraperAPI: 0.5‚Ç¨ par 1000 req = ~2‚Ç¨/mois (200 req = 101 CNKs √ó 2 sites)
- Playwright: Gratuit (RAM)
- Total: ~2‚Ç¨/mois

---

### Strat√©gie D: "Datacenter Belgique" (Optimal)
**Cible**: Tous les sites
**Temps**: 1-2 min | **Co√ªt**: 5‚Ç¨/mois | **Risque**: Tr√®s faible

```
Au lieu de proxies externes:
1. Louer VPS OVH/DigitalOcean en Belgique (~5‚Ç¨/mois)
2. Lancer scraper depuis la VM
3. Tous les sites voient une IP r√©sidentielle belgique
4. Pas besoin de proxies payants
5. Plus rapide que ScraperAPI
```

**Exemple OVH**:
- VPS 1GB RAM: 5‚Ç¨/mois
- Localisation: Belgique/Pays-Bas
- Acc√®s SSH: Oui

---

## üõ†Ô∏è Impl√©mentation Recommand√©e {#implementation}

### Phase 1: Aujourd'hui (Gratuit)
```python
# Ajouter au scraper.py

import random
from collections import deque

# Rotation d'User-Agents compl√®te
USER_AGENTS_EXTENDED = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
]

ACCEPT_LANGUAGES = [
    "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
    "fr;q=0.9,en-US;q=0.8,en;q=0.7",
    "en-US,en;q=0.9,fr;q=0.8",
]

def rotate_headers():
    """G√©n√®re des headers r√©alistes et variables"""
    return {
        "User-Agent": random.choice(USER_AGENTS_EXTENDED),
        "Accept-Language": random.choice(ACCEPT_LANGUAGES),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0",
    }

def human_like_delay():
    """D√©lais r√©alistes avec comportement humain"""
    choice = random.random()
    if choice < 0.05:  # 5%: pause distraction
        return random.uniform(10, 30)
    elif choice < 0.15:  # 10%: pause moyenne
        return random.uniform(3, 8)
    else:  # 85%: d√©lai normal
        return random.uniform(0.5, 2)
```

**Co√ªt**: 0‚Ç¨ | **Gain attendu**: +5% de pas de blocages

---

### Phase 2: Si toujours bloqu√© (5‚Ç¨ + temps)
```bash
# Option A: Louer VPS Belgique (recommand√©)
# Chez OVH: VPS 1GB √† 5‚Ç¨/mois
# SSH dessus et relancer le scraper

# Option B: Ajouter ScraperAPI (code minimal)
pip install requests

# Dans le scraper:
def scrape_farmaline_with_proxy(url):
    payload = {
        'api_key': 'votre_cl√©_scraper_api',
        'url': url,
    }
    return requests.get('http://api.scraperapi.com', params=payload)
```

**Co√ªt**: 5‚Ç¨/mois (VPS) ou ~2‚Ç¨/mois (ScraperAPI)
**Gain attendu**: 95%+ de succ√®s

---

### Phase 3: Si vraiment n√©cessaire (Playwright)
```bash
pip install playwright
playwright install chromium
```

**Co√ªt**: 0‚Ç¨ | **Perf**: -80% (tr√®s lent)

---

## üìä Monitoring et Alertes {#monitoring}

### Tracker les erreurs
```python
error_tracker = {
    'farmaline_403': 0,
    'newpharma_403': 0,
    'medi_market_429': 0,
    'multipharma_429': 0,
}

def log_error(site, error_code):
    error_tracker[f'{site}_{error_code}'] += 1
    
    if error_tracker[f'{site}_{error_code}'] > 3:
        print(f"‚ö†Ô∏è ALERTE: {site} bloque ({error_code})")
        # Envoyer email, Slack, etc.

# √Ä la fin du scraping
print("\nüìä R√©sum√© des erreurs:")
for error_type, count in error_tracker.items():
    if count > 0:
        print(f"  {error_type}: {count}")
```

### Dashboard simple
```python
# Ajouter au README output
def print_health_report():
    print("""
    üè• SCRAPER HEALTH REPORT
    ========================
    Medi-Market:  ‚úÖ 91/101 (OK)
    Multipharma:  ‚úÖ 99/101 (OK)
    Farmaline:    ‚ö†Ô∏è  0/101 (BLOCKED - 403)
    NewPharma:    ‚ö†Ô∏è  0/101 (BLOCKED - 403)
    
    ACTIONS RECOMMAND√âES:
    1. Ajouter User-Agent rotation (gratuit)
    2. Impl√©menter d√©lais humains (gratuit)
    3. VPS Belgique ou ScraperAPI (5-10‚Ç¨/mois)
    """)
```

---

## üéØ Tableau D√©cisionnel

| Situation | Action | Co√ªt | Temps |
|-----------|--------|------|-------|
| Medi-Market OK | Garder config actuelle | 0‚Ç¨ | 1 min |
| Multipharma OK | Garder config actuelle | 0‚Ç¨ | 1 min |
| Farmaline bloque | Phase 1 (User-Agents) | 0‚Ç¨ | 30 min |
| NewPharma bloque | Phase 1 (User-Agents) | 0‚Ç¨ | 30 min |
| Toujours bloqu√© | Phase 2 (VPS/ScraperAPI) | 5‚Ç¨ | 1h |
| Pages complexes | Phase 3 (Playwright) | 0‚Ç¨ | 2h |
| Maximum fiabilit√© | Phase 2 + Dashboard | 5‚Ç¨ | 1h |

---

## ‚öñÔ∏è Consid√©rations L√©gales

- **Robots.txt**: V√©rifier que les sites n'interdisent pas le scraping
- **Terms of Service**: Lire les conditions (possibilit√© de mentions)
- **Respect**: D√©lais suffisants pour ne pas surcharger les serveurs
- **API officielle**: Contacter d'abord pour une int√©gration l√©gale

**Statut l√©gal du scraping en Belgique**:
- ‚úÖ L√©gal si respecte les serveurs et pas violation TOS
- ‚ùå Ill√©gal si bypasse authentification ou viole TOS explicitement
- ‚ö†Ô∏è Gris si les donn√©es sont publiques mais site refuse

---

## üìö Ressources

### Documentation
- Playwright: https://playwright.dev/
- ScraperAPI: https://www.scraperapi.com/
- Bright Data: https://brightdata.com/
- Cloudscraper: https://github.com/VenoM1337/cloudscraper

### Outils
- cURL headers tester: https://curl.se/
- User-Agent checker: https://www.whoishostingthis.com/tools/user-agent/
- IP check: https://www.whatismyipaddress.com/

### Communaut√©
- Stack Overflow: tag `web-scraping`
- Reddit: r/webscraping
- GitHub: awesome-web-scraping

---

## üöÄ Prochaines √âtapes

1. **Aujourd'hui**: Impl√©menter Phase 1 (User-Agents, d√©lais humains)
2. **Semaine prochaine**: Tester avec Phase 1, mesurer r√©sultats
3. **Si encore bloqu√©**: √âvaluer Phase 2 (VPS ou ScraperAPI)
4. **Monitoring**: Ajouter dashboard pour tracker la fiabilit√©

---

**Derni√®re mise √† jour**: 18 Oct 2025
**Auteur**: Diego Claes - LP_Pharma Project
